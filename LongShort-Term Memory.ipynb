{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194ced5c-6e9e-41e0-a932-efde2c5dad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 17s 1us/step\n",
      "\n",
      "Training with SimpleRNN...\n",
      "Epoch 1/2\n",
      "313/313 - 33s - loss: 0.5065 - accuracy: 0.7375 - val_loss: 0.3732 - val_accuracy: 0.8442 - 33s/epoch - 104ms/step\n",
      "Epoch 2/2\n",
      "313/313 - 31s - loss: 0.3254 - accuracy: 0.8607 - val_loss: 0.4267 - val_accuracy: 0.8254 - 31s/epoch - 98ms/step\n",
      "\n",
      "Training with GRU...\n",
      "Epoch 1/2\n",
      "313/313 - 68s - loss: 0.4572 - accuracy: 0.7686 - val_loss: 0.3611 - val_accuracy: 0.8422 - 68s/epoch - 216ms/step\n",
      "Epoch 2/2\n",
      "313/313 - 64s - loss: 0.2105 - accuracy: 0.9204 - val_loss: 0.3243 - val_accuracy: 0.8672 - 64s/epoch - 206ms/step\n",
      "\n",
      "Training with LSTM...\n",
      "Epoch 1/2\n",
      "313/313 - 75s - loss: 0.4066 - accuracy: 0.8095 - val_loss: 0.3050 - val_accuracy: 0.8702 - 75s/epoch - 239ms/step\n",
      "Epoch 2/2\n",
      "313/313 - 73s - loss: 0.2022 - accuracy: 0.9240 - val_loss: 0.3118 - val_accuracy: 0.8730 - 73s/epoch - 232ms/step\n",
      "\n",
      "Final Accuracy Comparison:\n",
      "SimpleRNN: 0.8242\n",
      "GRU: 0.8511\n",
      "LSTM: 0.8649\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load IMDB dataset\n",
    "max_features = 20000  # number of words\n",
    "maxlen = 200          # cut reviews after 200 words\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "def build_model(rnn_layer):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(max_features, 128),\n",
    "        rnn_layer,\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Try different recurrent layers\n",
    "for rnn_layer in [\n",
    "    layers.SimpleRNN(64),\n",
    "    layers.GRU(64),\n",
    "    layers.LSTM(64),\n",
    "]:\n",
    "    print(f\"\\nTraining with {rnn_layer.__class__.__name__}...\")\n",
    "    model = build_model(rnn_layer)\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=64,\n",
    "        epochs=2,          # keep small for runtime\n",
    "        validation_split=0.2,\n",
    "        verbose=2\n",
    "    )\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    results[rnn_layer.__class__.__name__] = score[1]\n",
    "\n",
    "print(\"\\nFinal Accuracy Comparison:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e97958-5b64-47e7-b76f-5127610fc7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, None, 40, 40, 40   59200     \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, None, 40, 40, 40   160       \n",
      " Normalization)              )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, None, 40, 40, 40   115360    \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, None, 40, 40, 40   160       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, None, 40, 40, 1)   1081      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175961 (687.35 KB)\n",
      "Trainable params: 175801 (686.72 KB)\n",
      "Non-trainable params: 160 (640.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D\n",
    "\n",
    "# Example ConvLSTM2D model\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(None, 40, 40, 1), padding=\"same\", return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5507b346-03bb-47ef-9ecb-0403b318e8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2110848/2110848 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "max_features = 10000  # Number of words to keep\n",
    "maxlen = 500          # Cut texts after 500 words\n",
    "\n",
    "# Load Reuters dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.reuters.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences so all have same length\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8870b160-3a17-4115-9780-7e6108e2f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm():\n",
    "    model = keras.Sequential([ layers.Embedding(max_features, 128, input_length=maxlen), layers.LSTM(64), layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869d4c19-94c7-486a-a247-8b07b8458068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv1d():\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(max_features, 128, input_length=maxlen),\n",
    "        layers.Conv1D(64, 5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, 5, activation=\"relu\"),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65f4c0e-cf00-4cc5-8934-a7f7d224fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n",
      "Epoch 1/3\n",
      "113/113 - 69s - loss: 2.4321 - accuracy: 0.4274 - val_loss: 2.0324 - val_accuracy: 0.4841 - 69s/epoch - 608ms/step\n",
      "Epoch 2/3\n",
      "113/113 - 68s - loss: 1.8240 - accuracy: 0.5254 - val_loss: 1.6752 - val_accuracy: 0.5804 - 68s/epoch - 605ms/step\n",
      "Epoch 3/3\n",
      "113/113 - 75s - loss: 1.6605 - accuracy: 0.5550 - val_loss: 1.6487 - val_accuracy: 0.5732 - 75s/epoch - 664ms/step\n",
      "\n",
      "Training Conv1D...\n",
      "Epoch 1/3\n",
      "113/113 - 37s - loss: 2.3251 - accuracy: 0.4525 - val_loss: 1.8283 - val_accuracy: 0.5264 - 37s/epoch - 326ms/step\n",
      "Epoch 2/3\n",
      "113/113 - 35s - loss: 1.6585 - accuracy: 0.5797 - val_loss: 1.5426 - val_accuracy: 0.6155 - 35s/epoch - 306ms/step\n",
      "Epoch 3/3\n",
      "113/113 - 34s - loss: 1.3504 - accuracy: 0.6786 - val_loss: 1.3348 - val_accuracy: 0.6917 - 34s/epoch - 304ms/step\n",
      "\n",
      "Final Test Accuracy Comparison:\n",
      "LSTM: 0.5726\n",
      "Conv1D: 0.6754\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm = build_lstm()\n",
    "lstm.fit(x_train, y_train,  batch_size=64, epochs=3,  validation_split=0.2, verbose=2)\n",
    "score = lstm.evaluate(x_test, y_test, verbose=0)\n",
    "results[\"LSTM\"] = score[1]\n",
    "\n",
    "# Train Conv1D\n",
    "print(\"\\nTraining Conv1D...\")\n",
    "conv = build_conv1d()\n",
    "conv.fit(x_train, y_train, batch_size=64, epochs=3, validation_split=0.2, verbose=2)\n",
    "score = conv.evaluate(x_test, y_test, verbose=0)\n",
    "results[\"Conv1D\"] = score[1]\n",
    "\n",
    "print(\"\\nFinal Test Accuracy Comparison:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7630126-b392-47b1-a972-1952de740fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
      "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
      "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
      "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
      "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
      "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
      "\n",
      "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
      "0          3.33          3.11          0.22       1.94             3.12   \n",
      "1          3.23          3.02          0.21       1.89             3.03   \n",
      "2          3.21          3.01          0.20       1.88             3.02   \n",
      "3          3.26          3.07          0.19       1.92             3.08   \n",
      "4          3.27          3.08          0.19       1.92             3.09   \n",
      "\n",
      "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0       1307.75      1.03           1.75     152.3  \n",
      "1       1309.80      0.72           1.50     136.1  \n",
      "2       1310.24      0.19           0.63     171.6  \n",
      "3       1309.19      0.34           0.50     198.0  \n",
      "4       1309.00      0.32           0.63     214.3  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Download dataset\n",
    "fname = keras.utils.get_file(\"jena_climate.csv.zip\", url, extract=True)\n",
    "\n",
    "\n",
    "# Load into dataframe\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ecf661-9441-49c5-9344-06ff0c047acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy (excluding the 'Date Time' column)\n",
    "data = df.iloc[:, 1:].values\n",
    "\n",
    "# Standardize (normalize)\n",
    "mean = data[:200000].mean(axis=0)\n",
    "std = data[:200000].std(axis=0)\n",
    "data = (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae339d0-e3ce-483a-b624-f6cfb65e4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lookback = 720     # how many past timesteps (10 min * 720 = 5 days)\n",
    "step = 6           # sample one data point every hour\n",
    "delay = 144        # predict 24 hours in the future\n",
    "batch_size = 128\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]  # column 1 = temp in dataset\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb2825f-6624-4ebc-96de-97a03ae58738",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(data, lookback=lookback, delay=delay, min_index=0, max_index=200000, shuffle=True, step=step, batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(data, lookback=lookback, delay=delay, min_index=200001, max_index=300000, step=step, batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(data, lookback=lookback, delay=delay, min_index=300001, max_index=None, step=step, batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "test_steps = (len(data) - 300001 - lookback) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3763f0f7-ebd1-44aa-ae73-443f4f526c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.LSTM(32, input_shape=(lookback // step, data.shape[-1])), layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dff754e-6e15-430c-9c30-f6f822c5024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 55s 104ms/step - loss: 0.3039 - val_loss: 0.2746\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.2778 - val_loss: 0.2685\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.2653 - val_loss: 0.2715\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.2563 - val_loss: 0.2749\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 54s 107ms/step - loss: 0.2469 - val_loss: 0.2774\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.2404 - val_loss: 0.2783\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.2333 - val_loss: 0.2850\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 53s 107ms/step - loss: 0.2289 - val_loss: 0.2841\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 53s 107ms/step - loss: 0.2249 - val_loss: 0.2898\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 56s 113ms/step - loss: 0.2195 - val_loss: 0.2934\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 58s 117ms/step - loss: 0.2136 - val_loss: 0.3024\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 56s 112ms/step - loss: 0.2098 - val_loss: 0.2996\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 55s 109ms/step - loss: 0.2045 - val_loss: 0.3050\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.2018 - val_loss: 0.3005\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 54s 107ms/step - loss: 0.1989 - val_loss: 0.3088\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 53s 107ms/step - loss: 0.1956 - val_loss: 0.3112\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 54s 107ms/step - loss: 0.1918 - val_loss: 0.3049\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1874 - val_loss: 0.3154\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 59s 118ms/step - loss: 0.1852 - val_loss: 0.3087\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.1822 - val_loss: 0.3183\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, steps_per_epoch=500, epochs=20, validation_data=val_gen, validation_steps=val_steps)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1972cb4b-ad0c-47aa-9216-a68bfbe59ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936/936 [==============================] - 29s 31ms/step - loss: 0.3366\n",
      "Test MAE: 0.33657678961753845\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(test_gen, steps=test_steps)\n",
    "print(\"Test MAE:\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
