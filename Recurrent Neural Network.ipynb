{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82763e22-2280-4301-a4ab-c57f353a10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a615a-78db-4e88-897d-7f102fe8c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, activation=np.tanh):\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activation = activation\n",
    "\n",
    "        #Initializing weights\n",
    "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
    "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
    "        self.b = np.zeros(n_nodes)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        batch_size, n_sequences, _ = x.shape\n",
    "\n",
    "        #Initializing hidden state\n",
    "        if h0 is None:\n",
    "            h_t = np.zeros((batch_size, self.n_nodes))\n",
    "        else:\n",
    "            h_t = h0\n",
    "\n",
    "        #Stored states for all timesteps\n",
    "        self.h_list = []\n",
    "\n",
    "        for t in range(n_sequences):\n",
    "            x_t = x[:, t, :]  # (batch_size, n_features)\n",
    "\n",
    "            a_t = x_t @ self.Wx + h_t @ self.Wh + self.b\n",
    "            h_t = self.activation(a_t)\n",
    "\n",
    "            self.h_list.append(h_t)\n",
    "\n",
    "        #Final hidden state\n",
    "        return h_t, self.h_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b815752-2364-46fc-85d4-f603e16257c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h0 = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a9ed33-1ea1-4daa-be27-312e37b5ffe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c52b0c2-cbb6-4493-855f-33773f27083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden state:\n",
      " [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(n_features, n_nodes)\n",
    "rnn.Wx = w_x\n",
    "rnn.Wh = w_h\n",
    "rnn.b = b\n",
    "\n",
    "h_final, h_list = rnn.forward(x, h0)\n",
    "print(\"Final hidden state:\\n\", h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c3a21f-03ba-4939-a81d-90f256e8fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, activation=np.tanh):\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activation = activation\n",
    "\n",
    "        #Parameters\n",
    "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
    "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
    "        self.b = np.zeros(n_nodes)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        batch_size, n_sequences, _ = x.shape\n",
    "\n",
    "        if h0 is None:\n",
    "            h_t = np.zeros((batch_size, self.n_nodes))\n",
    "        else:\n",
    "            h_t = h0\n",
    "\n",
    "        self.h_list = [h_t]\n",
    "        self.a_list = []\n",
    "        self.x = x\n",
    "\n",
    "        for t in range(n_sequences):\n",
    "            x_t = x[:, t, :]  # (batch_size, n_features)\n",
    "\n",
    "            a_t = x_t @ self.Wx + h_t @ self.Wh + self.b\n",
    "            h_t = self.activation(a_t)\n",
    "\n",
    "            self.a_list.append(a_t)\n",
    "            self.h_list.append(h_t)\n",
    "\n",
    "        return h_t, self.h_list[1:]  # last hidden + all h_t\n",
    "\n",
    "    def backward(self, dh_last, learning_rate=0.01):\n",
    "        batch_size, n_sequences, _ = self.x.shape\n",
    "\n",
    "        #Initializing grads\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        dx = np.zeros_like(self.x)\n",
    "        dh_next = dh_last  # starts from loss gradient at last timestep\n",
    "\n",
    "        #Looping backwards through time\n",
    "        for t in reversed(range(n_sequences)):\n",
    "            a_t = self.a_list[t]\n",
    "            h_prev = self.h_list[t]\n",
    "            x_t = self.x[:, t, :]\n",
    "\n",
    "            #Gradient wrt pre-activation\n",
    "            dtanh = (1 - np.tanh(a_t) ** 2) * dh_next\n",
    "\n",
    "            #Accumulate grads\n",
    "            dWx += x_t.T @ dtanh\n",
    "            dWh += h_prev.T @ dtanh\n",
    "            db += dtanh.sum(axis=0)\n",
    "\n",
    "            # Gradients to pass backward\n",
    "            dx[:, t, :] = dtanh @ self.Wx.T\n",
    "            dh_next = dtanh @ self.Wh.T\n",
    "\n",
    "        # Normalize by batch size\n",
    "        dWx /= batch_size\n",
    "        dWh /= batch_size\n",
    "        db /= batch_size\n",
    "\n",
    "        # Update params\n",
    "        self.Wx -= learning_rate * dWx\n",
    "        self.Wh -= learning_rate * dWh\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "        return dx, dh_next, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223351f-57db-4a58-8759-09727ecd4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
