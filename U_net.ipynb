{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFAWeQnCw6enXSOrVruGin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/U_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1"
      ],
      "metadata": {
        "id": "fseXn42-EN8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zhixuhao/unet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6dbDo5LD6uw",
        "outputId": "6d10d8f1-cffc-4cb5-d6bd-4f7b0f5cda19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'unet' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/unet')"
      ],
      "metadata": {
        "id": "HW4SAHQbETW4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/from keras.preprocessing.image/from tensorflow.keras.preprocessing.image/' /content/unet/data.py\n"
      ],
      "metadata": {
        "id": "BQdXJGRgPxVY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Model(input =/Model(inputs=/g' /content/unet/model.py\n",
        "!sed -i 's/output =/outputs=/g' /content/unet/model.py\n"
      ],
      "metadata": {
        "id": "TSsxuoRIQAlI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Adam(lr =/Adam(learning_rate=/g' /content/unet/model.py\n"
      ],
      "metadata": {
        "id": "7ftOttAtQL__"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i \"s/unet_membrane.hdf5/unet_membrane.keras/g\" /content/unet/main.py\n"
      ],
      "metadata": {
        "id": "MlU-X2K2QWil"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/fit_generator/fit/g' /content/unet/main.py\n"
      ],
      "metadata": {
        "id": "kqnMzSVRQlFp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/predict_generator/predict/g' /content/unet/main.py\n"
      ],
      "metadata": {
        "id": "veLmHAPuRHA_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/yield img/yield (img,)/g' /content/unet/data.py\n"
      ],
      "metadata": {
        "id": "mwsIQUNZRs7c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the saveResult function in data.py to handle float images\n",
        "!sed -i '/def saveResult(save_path, results):/,+6 s/imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)/from skimage import img_as_ubyte\\n        imsave(os.path.join(save_path,\"%d_predict.png\"%i), img_as_ubyte(img))/g' /content/unet/data.py\n"
      ],
      "metadata": {
        "id": "6iL8m_rJTASP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ],
      "metadata": {
        "id": "14Cs9q-jTCwH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a6c5907",
        "outputId": "d3103cdc-4775-4bcb-9afd-f0bcc6f4595c"
      },
      "source": [
        "from data import testGenerator\n",
        "import os\n",
        "\n",
        "# Re-initialize the test generator\n",
        "test_path = \"data/membrane/test\"\n",
        "testGene = testGenerator(test_path)\n",
        "\n",
        "# Get one batch from the test generator\n",
        "sample_batch = next(testGene)\n",
        "\n",
        "# Print the type and shape of the sample batch\n",
        "print(type(sample_batch))\n",
        "if isinstance(sample_batch, (list, tuple)):\n",
        "    for i, item in enumerate(sample_batch):\n",
        "        print(f\"Item {i}: Type - {type(item)}, Shape - {item.shape if hasattr(item, 'shape') else 'N/A'}\")\n",
        "else:\n",
        "    print(f\"Shape - {sample_batch.shape if hasattr(sample_batch, 'shape') else 'N/A'}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "Item 0: Type - <class 'numpy.ndarray'>, Shape - (1, 256, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46cdd50f"
      },
      "source": [
        "\n",
        "!sed -i 's/yield img/yield (img,)/g' /content/unet/data.py"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from model import unet\n",
        "from data import testGenerator, saveResult\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# Load your trained model\n",
        "model = unet()\n",
        "model.load_weights(\"unet_membrane.keras\")\n",
        "\n",
        "# Path to test images\n",
        "test_path = \"data/membrane/test\"\n",
        "\n",
        "# Create test data generator\n",
        "testGene = testGenerator(test_path)\n",
        "\n",
        "# Predict\n",
        "num_test_images = len(os.listdir(test_path))\n",
        "results = model.predict(testGene, steps=num_test_images, verbose=1)\n",
        "\n",
        "# Convert results to 8-bit before saving (avoids 'mode F' PNG error)\n",
        "results_uint8 = [img_as_ubyte(img) for img in results]\n",
        "\n",
        "# Save results\n",
        "saveResult(test_path, results_uint8)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Wd8CZsTw-e",
        "outputId": "24699c11-b0b4-44e6-ad99-2914624e51cb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 98 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data import trainGenerator, testGenerator\n",
        "from model import unet\n",
        "\n",
        "\n",
        "train_path = 'data/membrane/train'\n",
        "test_path = 'data/membrane/test'\n",
        "\n",
        "myGene = trainGenerator(batch_size=1, train_path=train_path,\n",
        "                        image_folder='image', mask_folder='label',\n",
        "                        aug_dict={}, target_size=(64,64))\n",
        "\n",
        "testGene = testGenerator(test_path, target_size=(64, 64))\n",
        "model = unet(input_size=(64,64,1))"
      ],
      "metadata": {
        "id": "QftJZioLWSw1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('unet_membrane.keras', monitor='loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(myGene, steps_per_epoch=30, epochs=1, callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAytYyKhWeN6",
        "outputId": "07585919-843b-4b2b-bc58-4f3fafa141ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30 images belonging to 1 classes.\n",
            "Found 30 images belonging to 1 classes.\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7289 - loss: 0.5763\n",
            "Epoch 1: loss improved from inf to 0.54068, saving model to unet_membrane.keras\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 167ms/step - accuracy: 0.7301 - loss: 0.5751\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f92500be960>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(num_test_images):\n",
        "    img = next(testGene)\n",
        "    pred = model.predict(img)  # shape: (1,64,64,1)\n",
        "    results.append(pred[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "HVQ-jd3yWwY0",
        "outputId": "4052dffc-11e0-47ad-fe83-5e948084f892"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: keras_tensor_78\n",
            "Received: inputs=('Tensor(shape=(1, 64, 64, 1))',)\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-800591694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_test_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestGene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (1,64,64,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg0GAkc9F7cm",
        "outputId": "dca16ee3-2361-4d69-e03f-bf00e57a0aad"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-30 19:01:45.500112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756580505.520528   11686 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756580505.526851   11686 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756580505.542659   11686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756580505.542693   11686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756580505.542699   11686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756580505.542705   11686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-30 19:01:50.402900: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1756580510.403056   11686 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 705 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Found 30 images belonging to 1 classes.\n",
            "Found 30 images belonging to 1 classes.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1756580517.302469   11724 service.cc:152] XLA service 0x7ba1b002c910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1756580517.302502   11724 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1756580518.512181   11724 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-08-30 19:01:59.600530: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:01:59.905416: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.111314: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.253932: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 296.50MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.254010: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.344030: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.344108: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 572.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.482373: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.31GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.482443: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-08-30 19:02:00.573523: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "I0000 00:00:1756580537.016356   11724 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "2025-08-30 19:02:27.022666: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 947.24MiB (rounded to 993252352)requested by op \n",
            "2025-08-30 19:02:27.022989: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] ___________________________***_________________________________*****************xx********x*********\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/unet/main.py\", line 18, in <module>\n",
            "    model.fit(myGene,steps_per_epoch=300,epochs=1,callbacks=[model_checkpoint])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n",
            "\n",
            "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
            "  File \"/content/unet/main.py\", line 18, in <module>\n",
            "\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n",
            "\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
            "\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n",
            "\n",
            "Out of memory while trying to allocate 993252200 bytes.\n",
            "\t [[{{node StatefulPartitionedCall}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
            " [Op:__inference_multi_step_on_iterator_9292]\n",
            "2025-08-30 19:02:27.585660: W tensorflow/core/kernels/data/generator_dataset_op.cc:109] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.\n",
            "\t [[{{node PyFunc}}]]\n",
            "F0000 00:00:1756580558.798084   11686 bfc_allocator.cc:816] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) \n",
            "*** Check failure stack trace: ***\n",
            "    @     0x7ba2c5a34194  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n",
            "    @     0x7ba2c5a33b34  absl::lts_20230802::log_internal::LogMessage::Flush()\n",
            "    @     0x7ba2c5a34629  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n",
            "    @     0x7ba2c4e853a2  tsl::BFCAllocator::DeallocateRawInternal()\n",
            "    @     0x7ba2c4e84c02  tsl::BFCAllocator::DeallocateRaw()\n",
            "    @     0x7ba2b5bba2d9  tensorflow::(anonymous namespace)::MlirTensorBuffer::~MlirTensorBuffer()\n",
            "    @     0x7ba2c4a33ddc  tensorflow::Tensor::~Tensor()\n",
            "    @     0x7ba2c48698b3  tensorflow::Var::~Var()\n",
            "    @     0x7ba2c4a6cd82  tensorflow::ResourceHandle::~ResourceHandle()\n",
            "    @     0x7ba2c4a4b088  tensorflow::(anonymous namespace)::Buffer<>::~Buffer()\n",
            "    @     0x7ba2c4a4b11e  tensorflow::(anonymous namespace)::Buffer<>::~Buffer()\n",
            "    @     0x7ba2c4a33ddc  tensorflow::Tensor::~Tensor()\n",
            "    @     0x7ba2ab61c1f1  std::__detail::__variant::__gen_vtable_impl<>::__visit_invoke()\n",
            "    @     0x7ba2ab61813a  tensorflow::TensorHandle::~TensorHandle()\n",
            "    @     0x7ba2ab6182fe  tensorflow::TensorHandle::~TensorHandle()\n",
            "    @     0x7ba2a80ea475  TFE_DeleteTensorHandle\n",
            "    @     0x7ba2bd12b21f  EagerTensor_dealloc\n",
            "    @           0x53b4b4  (unknown)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import *\n",
        "from data import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint # Import ModelCheckpoint\n",
        "\n",
        "data_gen_args = dict(rotation_range=0.2, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, zoom_range=0.05, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "myGene = trainGenerator(20, 'data/membrane/train', 'image', 'label', data_gen_args, save_to_dir = None)\n",
        "\n",
        "model = unet()\n",
        "model_checkpoint = ModelCheckpoint('unet_salt.keras', monitor='loss', verbose=1, save_best_only=True) # Changed extension to .keras\n",
        "\n",
        "model.fit(myGene, steps_per_epoch=10, epochs=5, callbacks=[model_checkpoint]) # Changed fit_generator to fit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m8voL84GF6G",
        "outputId": "3074dd7d-f862-458e-b8f2-d307d6887549"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30 images belonging to 1 classes.\n",
            "Found 30 images belonging to 1 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.7602 - loss: 0.5403\n",
            "Epoch 1: loss improved from inf to 0.50770, saving model to unet_salt.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 9s/step - accuracy: 0.7612 - loss: 0.5373\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930ms/step - accuracy: 0.7896 - loss: 0.4293\n",
            "Epoch 2: loss improved from 0.50770 to 0.42098, saving model to unet_salt.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.7899 - loss: 0.4285  \n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948ms/step - accuracy: 0.8017 - loss: 0.3847\n",
            "Epoch 3: loss improved from 0.42098 to 0.37548, saving model to unet_salt.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.8024 - loss: 0.3839  \n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942ms/step - accuracy: 0.8336 - loss: 0.3458\n",
            "Epoch 4: loss improved from 0.37548 to 0.34488, saving model to unet_salt.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.8338 - loss: 0.3457  \n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.8453 - loss: 0.3314\n",
            "Epoch 5: loss improved from 0.34488 to 0.32717, saving model to unet_salt.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.8455 - loss: 0.3310  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f9250465070>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data import testGenerator, saveResult\n",
        "from model import unet\n",
        "import os\n",
        "from skimage import img_as_ubyte # Import img_as_ubyte\n",
        "\n",
        "gene = testGenerator('data/membrane/test', target_size=(64, 64))\n",
        "unet = unet(input_size=(64,64,1))\n",
        "unet.load_weights('unet_salt.keras')\n",
        "pred = unet.predict(gene, steps=len(os.listdir('data/membrane/test')), verbose=1)\n",
        "\n",
        "# Convert predictions to 8-bit unsigned integer format and split into a list of individual images\n",
        "pred_uint8 = img_as_ubyte(pred)\n",
        "pred_list = [pred_uint8[i] for i in range(pred_uint8.shape[0])] # Split the batch into a list of images\n",
        "\n",
        "saveResult(\"data/membrane/test\", pred_list) # Pass the list of images to saveResult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnatz4lrGaQb",
        "outputId": "ae0daaee-9494-4495-f940-bba5ac6f052c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 98 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "ZyocSs8dGc-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net: Simple Explanation\n",
        "\n",
        "**Reference:**  \n",
        "O. Ronneberger, P. Fischer, T. Brox. *U-Net: Convolutional Networks for Biomedical Image Segmentation.* MICCAI 2015.\n",
        "\n",
        "\n",
        "\n",
        "## What U-Net Does\n",
        "- U-Net is a **neural network for image segmentation**.  \n",
        "- Segmentation means labeling each pixel in an image (e.g., which pixels belong to a cell).  \n",
        "- It works especially well for **biomedical images** where datasets are small.\n",
        "\n",
        "\n",
        "\n",
        "## How U-Net Works\n",
        "\n",
        "### 1. Encoder (Contracting Path)\n",
        "- Think of this as the network **looking at the big picture**.  \n",
        "- Uses **convolutions + pooling** to shrink the image but keep important features.  \n",
        "\n",
        "### 2. Bottleneck\n",
        "- The **smallest, most compressed version** of the image features.  \n",
        "- Acts as a bridge between understanding the big picture and rebuilding the image.\n",
        "\n",
        "### 3. Decoder (Expanding Path)\n",
        "- **Upsamples** the features to the original image size.  \n",
        "- Uses **skip connections** from the encoder to keep details.  \n",
        "- Combines **high-level ideas** (from encoder) with **low-level details** for precise segmentation.\n",
        "\n",
        "\n",
        "\n",
        "## Key Ideas Made Simple\n",
        "- **Skip connections**: help the network remember details while learning global context.  \n",
        "- **Works with small data**: uses **data augmentation** like rotation and elastic deformations.  \n",
        "- **End-to-end training**: network learns directly from images to pixel labels.  \n",
        "\n",
        "\n",
        "\n",
        "## Why It’s Useful\n",
        "- Segments images **accurately**, even with few training examples.  \n",
        "- Fast and efficient, so practical for real biomedical applications.  \n",
        "- Forms the base for many newer segmentation models like **3D U-Net** or **Attention U-Net**.\n",
        "\n",
        "\n",
        "## Overall\n",
        "This paper presents a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.\n",
        "\n",
        "This network is based on a fully convolutional network, It also uses many layers and in this network pooling operations are replaced with upsampling operators."
      ],
      "metadata": {
        "id": "sA-ZBHn_GfLD"
      }
    }
  ]
}